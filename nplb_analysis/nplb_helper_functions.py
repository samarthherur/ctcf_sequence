import os
import argparse
import pandas as pd
import numpy as np
from scipy.stats import hypergeom
import matplotlib.pyplot as plt
import json
import pyBigWig


def parse_nplb_train_args():
    """
    Parse arguments for training: only input FASTA and output prefix.
    """
    parser = argparse.ArgumentParser(
        description="Run promoterLearn training on neighbourhood FASTA"
    )
    # parser.add_argument(
    #     "--fasta", "-f",
    #     required=True,
    #     help="Input FASTA file (neighbourhood sequences) for training"
    # )
    parser.add_argument(
        "--output_prefix", "-o",
        required=True,
        help="Output directory of NPLB training results, which includes architectureDetails.txt"
    )
    return parser.parse_args()

def parse_nplb_classify_args():
    """
    Parse arguments for classification: input FASTA, model, output prefix,
    and a TSV mapping file with columns 'original_cluster_id', 'mapped_cluster_id', 'flip'.
    """
    parser = argparse.ArgumentParser(
        description="Run promoterClassify on neighbourhood FASTA with cluster mapping"
    )
    parser.add_argument(
        "--fasta", "-f",
        required=True,
        help="Input FASTA file (neighbourhood sequences) for classification"
    )
    parser.add_argument(
        "--model", "-m",
        required=True,
        help="Path to the trained promoterClassify model (.p file)"
    )
    parser.add_argument(
        "--output_prefix", "-o",
        required=True,
        help="Prefix (directory+basename) for all classification outputs"
    )
    parser.add_argument(
        "--cluster_map_tsv",
        required=False,
        default=None,
        help="Optional TSV file mapping original_cluster_id to mapped_cluster_id, with a 'flip' column"
    )
    return parser.parse_args()

# def run_promoter_learn(fasta_path: str, output_prefix: str):
#     cmd = f"promoterLearn -f {fasta_path} -o {output_prefix}"
#     print(f"Executing: {cmd}")
#     ret = os.system(cmd)
#     if ret != 0:
#         raise RuntimeError(f"promoterLearn failed with exit code {ret}")

# def run_promoter_classify(fasta_path: str, model_path: str, output_prefix: str):
#     """
#     Run promoterClassify on the given FASTA using a pre-trained model.
#     """
#     cmd = f"promoterClassify -f {fasta_path} -m {model_path} -o {output_prefix}"
#     print(f"Executing: {cmd}")
#     ret = os.system(cmd)
#     if ret != 0:
#         raise RuntimeError(f"promoterClassify failed with exit code {ret}")
    
    
###ordering functions
def preprocess_tss(in_bed: str) -> str:
    """Drop duplicates and sort TSS BED; return sorted path."""
    out = in_bed.replace('.bed', '_no_duplicates.bed')
    df = pd.read_csv(in_bed, sep='\t', header=None)
    # drop duplicates on columns 1 and 2
    df = df.drop_duplicates(subset=1).drop_duplicates(subset=2)
    df = df.sort_values(by=5)
    df.to_csv(out, sep='\t', header=False, index=False)
    sorted_out = out.replace('.bed', '_sorted.bed')
    os.system(f'bedtools sort -i {out} > {sorted_out}')
    return sorted_out

def compute_closest(nplb_bed: str, tss_sorted: str, out_bed: str) -> str:
    """Sort nplb_bed and compute closest to TSS; return path."""
    cmd = f'bedtools sort -i {nplb_bed} | bedtools closest -a - -b {tss_sorted} -D ref -t first > {out_bed}'
    os.system(cmd)
    return out_bed

def filter_columns_bed(in_bed: str, cols: list, out_bed: str) -> str:
    """Select only columns by zero-based index in cols."""
    df = pd.read_csv(in_bed, sep='\t', header=None)
    df = df.iloc[:, cols]
    df.to_csv(out_bed, sep='\t', header=False, index=False)
    return out_bed

def run_hypergeom_tests(df_bed: str, p_thresh: float):
    """Perform hypergeometric test per cluster; return DataFrame, clusters array, p-values."""
    data = pd.read_csv(df_bed, sep='\t', header=None)
    data.columns = ['Cluster','motif_direction','gene_direction','Distance']
    total = len(data)
    clusters = np.sort(data['Cluster'].unique())
    zero = data[data['Distance']==0]
    total_zero = len(zero)

    results = []
    for c in clusters:
        n_cluster = len(data[data['Cluster']==c])
        n_zero_cluster = len(zero[zero['Cluster']==c])
        pval = hypergeom.sf(n_zero_cluster-1, total, n_cluster, total_zero)
        results.append({
            'Cluster': c,
            'Total Entries in Cluster': n_cluster,
            'Total Zero Distance Entries': total_zero,
            'Zero Distance Entries in Cluster': n_zero_cluster,
            'Hypergeometric P-Value': pval
        })
    df_res = pd.DataFrame(results)
    p_vals = df_res['Hypergeometric P-Value'].values
    return df_res, clusters, p_vals

def plot_pvalue_heatmaps(clusters, p_vals, work_dir: str, p_thresh: float):
    """Plot raw, log, and color-coded heatmaps of p-values."""
    # raw
    fig, ax = plt.subplots()
    cax = ax.matshow(p_vals.reshape(1,-1), cmap='coolwarm', aspect='auto')
    fig.colorbar(cax)
    ax.set_xticks(range(len(clusters))); ax.set_xticklabels(clusters)
    ax.set_yticks([])
    plt.title('Hypergeometric P-Values')
    fig.savefig(os.path.join(work_dir, 'p_values_heatmap.png'))
    plt.close(fig)

    # log scale
    fig, ax = plt.subplots()
    cax = ax.matshow(-np.log10(p_vals).reshape(1,-1), cmap='coolwarm', aspect='auto')
    fig.colorbar(cax)
    ax.set_xticks(range(len(clusters))); ax.set_xticklabels(clusters)
    ax.set_yticks([])
    plt.title('-log10(P-Values)')
    fig.savefig(os.path.join(work_dir, 'log_p_values_heatmap.png'))
    plt.close(fig)

    # color-coded
    sig = clusters[p_vals < p_thresh]
    opp = clusters[p_vals > 1 - p_thresh]
    colors = ['red' if c in sig else 'blue' if c in opp else 'grey' for c in clusters]
    fig, ax = plt.subplots()
    cax = ax.matshow(p_vals.reshape(1,-1), cmap='coolwarm', aspect='auto')
    fig.colorbar(cax)
    ax.set_xticks(range(len(clusters))); ax.set_xticklabels(clusters)
    for i, lbl in enumerate(ax.get_xticklabels()):
        lbl.set_color(colors[i])
    ax.set_yticks([])
    plt.title('Color-coded P-Values')
    fig.savefig(os.path.join(work_dir, 'color_p_values_heatmap.png'))
    plt.close(fig)
    
# --- Helper functions for proximal binding ---
def extend_cluster_windows(bed_file: str, output_dir: str, window: int) -> str:
    """
    Extend each interval in the BED by +/- window bp, floor at zero.
    Returns path to the extended BED.
    """
    import pandas as pd, os
    df = pd.read_csv(bed_file, sep='\t', header=None)
    df[1] = df[1] - window
    df[2] = df[2] + window
    df.loc[df[1] < 0, 1] = 0
    df.loc[df[2] < 0, 2] = 0
    out_bed = os.path.join(output_dir, 'nplb_clustered_extended.bed')
    df.to_csv(out_bed, sep='\t', header=False, index=False)
    return out_bed

def count_proximal_binding(extended_bed: str, original_bed: str, output_dir: str) -> str:
    """
    Count overlaps of extended regions with original BED.
    Returns path to the cluster-count BED.
    """
    import os, subprocess
    os.makedirs(output_dir, exist_ok=True)
    out_bed = os.path.join(output_dir, 'cluster_count.bed')
    cmd = f'bedtools intersect -a {extended_bed} -b {original_bed} -c > {out_bed}'
    subprocess.call(cmd, shell=True)
    return out_bed

def compute_average_proximal(count_bed: str) -> dict:
    """
    Read the cluster-count BED and compute average binding count per cluster.
    Returns a dict {cluster_id: avg_count}.
    """
    import pandas as pd
    df = pd.read_csv(count_bed, sep='\t', header=None)
    df.columns = ['chr','start','end','cluster_id','strand','cluster_count']
    return df.groupby('cluster_id')['cluster_count'].mean().to_dict()


##phastcons
def run_phastcons_average(phastcons_bw: str, bed_file: str, output_dir: str) -> str:
    """
    Compute per-region phastCons scores using pyBigWig.
    Returns the path to the output TSV table with columns: cluster_id, mean_phastcons.
    """
    import os
    import pandas as pd

    os.makedirs(output_dir, exist_ok=True)
    out_tab = os.path.join(output_dir, 'phastcons_over_clusters.tab')

    # Load regions
    df = pd.read_csv(
        bed_file,
        sep='\t',
        header=None,
        names=['chrom','start','end','cluster_id','strand']
    )

    # Open bigWig and compute mean score per region
    bw = pyBigWig.open(phastcons_bw)
    df['mean_phastcons'] = df.apply(
        lambda r: bw.stats(r['chrom'], int(r['start']), int(r['end']), type="mean")[0] or 0.0,
        axis=1
    )
    bw.close()

    # Save cluster_id and mean score
    df[['cluster_id','mean_phastcons']].to_csv(out_tab, sep='\t', index=False, header=False)
    return out_tab


def compute_average_phastcons(phast_tab: str, bed_file: str) -> dict:
    """
    Read bigWigAverageOverBed output and compute average phastCons per cluster.
    Returns a dict mapping cluster_id â†’ average phastCons score.
    """
    import pandas as pd

    # Load the phastCons summary table
    df = pd.read_csv(phast_tab, sep='\t', header=None)

    # Reload the original BED to get cluster IDs in the same order
    df_regions = pd.read_csv(
        bed_file,
        sep='\t',
        header=None,
        names=['chr', 'start', 'end', 'cluster_id', 'strand']
    )

    # Append the cluster_id column
    df['cluster_id'] = df_regions['cluster_id']

    # Column index 4 (0-based) contains the "mean0" value from bigWigAverageOverBed
    avg_phast = df.groupby('cluster_id')[4].mean().to_dict()

    return avg_phast

# --- Function to update architectureDetails.txt based on mapping TSV ---
def update_architecture_details(base_dir: str, cluster_map_tsv: str):
    """
    Reads architectureDetails.txt in base_dir, applies the cluster_map_tsv mapping,
    and writes architectureDetails_updated.txt.
    """
    import os, csv, pandas as pd

    if not cluster_map_tsv:
        return

    # Load mapping from TSV
    cluster_mapping = {}
    with open(cluster_map_tsv, newline='') as mf:
        reader = csv.DictReader(mf, delimiter='\t')
        for row in reader:
            orig = str(row['original_cluster_id'])
            mapped = row['mapped_cluster_id']
            if mapped in (None, '', 'None'):
                mapped_val = None
            else:
                mapped_val = int(mapped)
            cluster_mapping[orig] = mapped_val

    arch_path = os.path.join(base_dir, 'architectureDetails.txt')
    if os.path.exists(arch_path):
        # Load raw architecture details
        arch = pd.read_csv(arch_path, sep='\t', header=None)

        # Map original IDs to new cluster IDs and drop unmapped rows
        mapped = arch.iloc[:, 0].astype(str).map(cluster_mapping)
        mapped = mapped.dropna().astype(int)

        # Filter arch to only rows with a valid mapped ID
        arch = arch.loc[mapped.index].copy()

        # Replace first column with integer cluster IDs
        arch.iloc[:, 0] = mapped.values

        # Write updated architecture details
        updated_path = os.path.join(base_dir, 'architectureDetails_updated.txt')
        arch.to_csv(updated_path, sep='\t', header=False, index=False)
        print(f"Saved updated architecture details to {updated_path}")
